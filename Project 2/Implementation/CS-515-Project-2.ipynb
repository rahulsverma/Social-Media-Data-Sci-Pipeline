{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ea6cf4",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3316e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d19670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604464e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from detoxify import Detoxify\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from pandas.core.groupby.groupby import DataFrame\n",
    "from transformers import TFAutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc00bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210054d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2fbd6f",
   "metadata": {},
   "source": [
    "# Twitter Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a10dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('twitter.csv')\n",
    "twitter_data['time_stamp'] = pd.to_datetime(twitter_data['time_stamp'], format=\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd529fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc151cb",
   "metadata": {},
   "source": [
    "## Tweets Toxicity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (twitter_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (twitter_data['time_stamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "t_data = twitter_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224af13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicityScore(row):\n",
    "    results = Detoxify('original').predict(row['text'].lower())\n",
    "    results.update((x, y*100) for x, y in results.items())\n",
    "    print(str(results) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65562702",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = t_data.apply(lambda row: toxicityScore(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453eaa9",
   "metadata": {},
   "source": [
    "## Tweets Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (twitter_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (twitter_data['time_stamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "t_data = twitter_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54da4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = t_data['context'].value_counts().rename_axis('Context').reset_index(name='Number of Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5152c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [9.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax =plt.subplots()\n",
    "sns.barplot(x=\"Number of Tweets\", y=\"Context\", data=t_data).set(title=\"Number of tweets per context\")\n",
    "fig.savefig('number-of-tweets-per-context.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c7138",
   "metadata": {},
   "source": [
    "## Top Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (twitter_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (twitter_data['time_stamp'] < datetime(2022, 11, 2, 0, 0, 0).isoformat())\n",
    "t_data = twitter_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = t_data.sample(n = 45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = t_data['text'].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower())))\\\n",
    "                         .sum(axis=0)\\\n",
    "                         .to_frame()\\\n",
    "                         .reset_index()\\\n",
    "                         .sort_values(by=0, ascending=False)\n",
    "hashtags.columns = ['Hashtag', 'Occurences']\n",
    "hashtags_1 = pd.concat([hashtags[:3], hashtags[4:6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (twitter_data['time_stamp'] >= datetime(2022, 11, 2, 0, 0, 0).isoformat()) & (twitter_data['time_stamp'] < datetime(2022, 11, 3, 0, 0, 0).isoformat())\n",
    "t_data = twitter_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = t_data['text'].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower() )))\\\n",
    "                         .sum(axis=0)\\\n",
    "                         .to_frame()\\\n",
    "                         .reset_index()\\\n",
    "                         .sort_values(by=0,ascending=False)\n",
    "hashtags.columns = ['Hashtag','Occurences']\n",
    "hashtags_2 = hashtags[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [9.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "sns.barplot(x=\"Occurences\", y=\"Hashtag\", data=hashtags_1,\n",
    "            ax=ax[0]).set(title=\"Top Hashtags on 11/01/2022\")\n",
    "sns.barplot(x=\"Occurences\", y=\"Hashtag\", data=hashtags_2,\n",
    "            ax=ax[1]).set(title=\"Top Hashtags on 11/02/2022\")\n",
    "fig.savefig('top-hashtags.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faac630",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ede3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (twitter_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (twitter_data['time_stamp'] < datetime(2022, 11, 3, 0, 0, 0).isoformat())\n",
    "t_data = twitter_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf406fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\",\n",
    "              \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "master_dict = {'I-LOCX': [], 'I-ORGX': [], 'I-PERX': [],\n",
    "               'B-LOCX': [], 'B-ORGX': [], 'B-PERX': []}\n",
    "word_temp = ''\n",
    "current_tag = ''\n",
    "old_tag = ''\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in t_data.index:\n",
    "    sequence = t_data['text'][ind].lower()\n",
    "    if len(sequence) > 512:\n",
    "        continue\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "    inputs = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)[0]\n",
    "    predictions = tf.argmax(outputs, axis=2)\n",
    "    list_bert = [(token, label_list[prediction])\n",
    "                 for token, prediction in zip(tokens, predictions[0].numpy())]\n",
    "    for i in list_bert:\n",
    "        if i[1] in ['O', 'B-MISC', 'I-MISC']:\n",
    "            if len(current_tag) > 0:\n",
    "                without_space_word = word_temp.strip()\n",
    "                if len(without_space_word) > 1:\n",
    "                    master_dict[current_tag + 'X'].append(without_space_word)\n",
    "            count = 0\n",
    "            word_temp = ''\n",
    "            current_tag = ''\n",
    "            continue\n",
    "        else:\n",
    "            current_tag = i[1]\n",
    "\n",
    "            if old_tag != current_tag and len(old_tag) > 0:\n",
    "                without_space_word = word_temp.strip()\n",
    "                if len(without_space_word) > 1:\n",
    "                    master_dict[old_tag + 'X'].append(without_space_word)\n",
    "                count = 0\n",
    "                word_temp = ''\n",
    "                current_tag = ''\n",
    "\n",
    "            if i[0].startswith('##'):\n",
    "                word_temp += i[0][2:].upper()\n",
    "            elif i[1] in ['I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-ORG', 'B-PER']:\n",
    "                word_temp += \" \" + i[0].upper()\n",
    "                current_tag = i[1]\n",
    "                count += 1\n",
    "            old_tag = current_tag\n",
    "\n",
    "    output['Location'] = list(master_dict['I-LOCX'] + master_dict['B-LOCX'])\n",
    "    output['Organization'] = list(\n",
    "        master_dict['I-ORGX'] + master_dict['B-ORGX'])\n",
    "    output['Person Name'] = list(master_dict['I-PERX'] + master_dict['B-PERX'])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('https')\n",
    "stopwords.add('t')\n",
    "stopwords.add('co')\n",
    "stopwords.add('rt')\n",
    "\n",
    "\n",
    "tokens = output['Location'] + output['Organization'] + output['Person Name']\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i].lower()\n",
    "\n",
    "comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800,\n",
    "                      background_color='white',\n",
    "                      stopwords=stopwords,\n",
    "                      min_font_size=10).generate(comment_words)\n",
    "\n",
    "plt.figure(figsize=(6, 6), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('tweets-wordcloud.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1af85d",
   "metadata": {},
   "source": [
    "# Subreddit Posts Comments Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615bd6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = pd.read_csv('reddit.csv')\n",
    "reddit_data['time_stamp'] = pd.to_datetime(reddit_data['time_stamp'], format=\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26dfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480ae38",
   "metadata": {},
   "source": [
    "## Subreddit Comments Toxicity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (reddit_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (reddit_data['time_stamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "r_data = reddit_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cd2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicityScore(row):\n",
    "    results = Detoxify('original').predict(row['text'].lower())\n",
    "    results.update((x, y*100) for x, y in results.items())\n",
    "    print(str(results) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed069aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = r_data.apply(lambda row: toxicityScore(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0b1ce",
   "metadata": {},
   "source": [
    "## Total Comments Per Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (reddit_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (reddit_data['time_stamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "r_data = reddit_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = r_data['subreddit'].value_counts().rename_axis('Subreddit').reset_index(name='Number of Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad1d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [9.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x=\"Subreddit\", y=\"Number of Comments\",\n",
    "                data=r_data, size=\"Number of Comments\").set(title=\"Number of comments per Subreddit\")\n",
    "fig.savefig('total-comments-per-subreddit.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93022f37",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861530e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (reddit_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (reddit_data['time_stamp'] < datetime(2022, 11, 3, 0, 0, 0).isoformat())\n",
    "r_data = reddit_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b68034",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "master_dict = {'I-LOCX': [], 'I-ORGX': [], 'I-PERX': [],\n",
    "               'B-LOCX': [], 'B-ORGX': [], 'B-PERX': []}\n",
    "word_temp = ''\n",
    "current_tag = ''\n",
    "old_tag = ''\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in r_data.index:\n",
    "    sequence = r_data['text'][ind].lower()\n",
    "    if len(sequence) > 512:\n",
    "        continue\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "    inputs = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)[0]\n",
    "    predictions = tf.argmax(outputs, axis=2)\n",
    "    list_bert = [(token, label_list[prediction])\n",
    "                 for token, prediction in zip(tokens, predictions[0].numpy())]\n",
    "    for i in list_bert:\n",
    "        if i[1] in ['O', 'B-MISC', 'I-MISC']:\n",
    "            if len(current_tag) > 0:\n",
    "                without_space_word = word_temp.strip()\n",
    "                if len(without_space_word) > 1:\n",
    "                    master_dict[current_tag + 'X'].append(without_space_word)\n",
    "            count = 0\n",
    "            word_temp = ''\n",
    "            current_tag = ''\n",
    "            continue\n",
    "        else:\n",
    "            current_tag = i[1]\n",
    "\n",
    "            if old_tag != current_tag and len(old_tag) > 0:\n",
    "                without_space_word = word_temp.strip()\n",
    "                if len(without_space_word) > 1:\n",
    "                    master_dict[old_tag + 'X'].append(without_space_word)\n",
    "                count = 0\n",
    "                word_temp = ''\n",
    "                current_tag = ''\n",
    "\n",
    "            if i[0].startswith('##'):\n",
    "                word_temp += i[0][2:].upper()\n",
    "            elif i[1] in ['I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-ORG', 'B-PER']:\n",
    "                word_temp += \" \" + i[0].upper()\n",
    "                current_tag = i[1]\n",
    "                count += 1\n",
    "            old_tag = current_tag\n",
    "\n",
    "    output['Location'] = list(master_dict['I-LOCX'] + master_dict['B-LOCX'])\n",
    "    output['Organization'] = list(\n",
    "        master_dict['I-ORGX'] + master_dict['B-ORGX'])\n",
    "    output['Person Name'] = list(master_dict['I-PERX'] + master_dict['B-PERX'])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10447728",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('https')\n",
    "stopwords.add('t')\n",
    "stopwords.add('co')\n",
    "stopwords.add('rt')\n",
    "\n",
    "\n",
    "tokens = output['Location'] + output['Organization'] + output['Person Name']\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i].lower()\n",
    "\n",
    "comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800,\n",
    "                      background_color='white',\n",
    "                      stopwords=stopwords,\n",
    "                      min_font_size=10).generate(comment_words)\n",
    "\n",
    "plt.figure(figsize=(6, 6), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('reddit-wordcloud.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe90681",
   "metadata": {},
   "source": [
    "# YouTube Videos Comments Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_data = pd.read_csv('youtube.csv')\n",
    "youtube_data['comment_timestamp'] = pd.to_datetime(youtube_data['comment_timestamp'], format=\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9131982",
   "metadata": {},
   "source": [
    "## YouTube Comments Toxicity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d79db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (youtube_data['comment_timestamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (youtube_data['comment_timestamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "y_data = youtube_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicityScore(row):\n",
    "    results = Detoxify('original').predict(row['comment_text'].lower())\n",
    "    results.update((x, y*100) for x, y in results.items())\n",
    "    print(str(results) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = y_data.apply(lambda row: toxicityScore(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c93e2fc",
   "metadata": {},
   "source": [
    "## Videos With Most Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (youtube_data['comment_timestamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (youtube_data['comment_timestamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "y_data = youtube_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y_data['video_title'].value_counts()[:10].rename_axis('Video').reset_index(name='Number of Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x=\"Number of Comments\", y=\"Video\", data=y_data).set(\n",
    "    title=\"Videos with most comments\")\n",
    "fig.savefig('videos-with-most-comments.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e6b48",
   "metadata": {},
   "source": [
    "## Authors With Most Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a39011",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (youtube_data['comment_timestamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (youtube_data['comment_timestamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "y_data = youtube_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8244f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y_data['author'].value_counts()[:10].rename_axis('Author').reset_index(name='Number of Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {'text.usetex': False, 'font.family': 'stixgeneral', 'mathtext.fontset': 'stix', })\n",
    "plt.rcParams[\"figure.figsize\"] = [15.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x=\"Number of Comments\", y=\"Author\", data=y_data).set(\n",
    "    title=\"Authors with most comments\")\n",
    "fig.savefig('authors-with-most-comments.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0e933",
   "metadata": {},
   "source": [
    "## Common Words In Comments Related To Scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a709be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (youtube_data['comment_timestamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (youtube_data['comment_timestamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "y_data = youtube_data.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('b')\n",
    "stopwords.add('br')\n",
    "\n",
    "for ind in y_data.index:\n",
    "    if 'telegram' in str(y_data['author'][ind]).lower() or 'whatsapp' in str(y_data['author'][ind]).lower():\n",
    "        sequence = str(y_data['comment_text'][ind]).lower()\n",
    "\n",
    "        tok = sequence.split(\" \")\n",
    "        tokens = [t for t in tok]\n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800,\n",
    "                      background_color='white',\n",
    "                      stopwords=stopwords,\n",
    "                      min_font_size=10).generate(comment_words)\n",
    "\n",
    "plt.figure(figsize=(6, 6), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('youtube-wordcloud.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f5b1d",
   "metadata": {},
   "source": [
    "# Common Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb268f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['time_stamp'] = pd.to_datetime(\n",
    "    twitter_data['time_stamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "mask = (twitter_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (\n",
    "    twitter_data['time_stamp'] < datetime(2022, 11, 5, 0, 0, 0).isoformat())\n",
    "t_data = twitter_data.loc[mask]\n",
    "t_data = t_data['time_stamp'].value_counts().rename_axis(\n",
    "    'Hours').reset_index(name='Number of Submissions')\n",
    "t_data = t_data.sort_values(by='Hours')\n",
    "t_data.reset_index(drop=True)\n",
    "t_data = t_data.groupby([t_data['Hours'].dt.hour])\n",
    "t_data = t_data[\"Number of Submissions\"].sum()\n",
    "t_data = DataFrame(t_data).reset_index()\n",
    "platform = ['twitter' for _ in range(len(t_data))]\n",
    "t_data['Platform'] = platform\n",
    "t_data['Hours'] = t_data['Hours'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410382e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['time_stamp'] = pd.to_datetime(\n",
    "    reddit_data['time_stamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "mask = (reddit_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (\n",
    "    reddit_data['time_stamp'] < datetime(2022, 11, 5, 0, 0, 0).isoformat())\n",
    "r_data = reddit_data.loc[mask]\n",
    "r_data = r_data['time_stamp'].value_counts().rename_axis(\n",
    "    'Hours').reset_index(name='Number of Submissions')\n",
    "r_data = r_data.sort_values(by='Hours')\n",
    "r_data.reset_index(drop=True)\n",
    "r_data = r_data.groupby([r_data['Hours'].dt.hour])\n",
    "r_data = r_data[\"Number of Submissions\"].sum()\n",
    "r_data = DataFrame(r_data).reset_index()\n",
    "platform = ['reddit' for _ in range(len(r_data))]\n",
    "r_data['Platform'] = platform\n",
    "r_data['Hours'] = r_data['Hours'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606425e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_data['comment_timestamp'] = pd.to_datetime(\n",
    "    youtube_data['comment_timestamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "mask = (youtube_data['comment_timestamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (\n",
    "    youtube_data['comment_timestamp'] < datetime(2022, 11, 5, 0, 0, 0).isoformat())\n",
    "y_data = youtube_data.loc[mask]\n",
    "y_data = y_data['comment_timestamp'].value_counts().rename_axis(\n",
    "    'Hours').reset_index(name='Number of Submissions')\n",
    "y_data = y_data.sort_values(by='Hours')\n",
    "y_data.reset_index(drop=True)\n",
    "y_data = y_data.groupby([y_data['Hours'].dt.hour])\n",
    "y_data = y_data[\"Number of Submissions\"].sum()\n",
    "y_data = DataFrame(y_data).reset_index()\n",
    "platform = ['youtube' for _ in range(len(y_data))]\n",
    "y_data['Platform'] = platform\n",
    "y_data['Hours'] = y_data['Hours'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([t_data, r_data, y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f51071",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10.00, 5.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=final, x=\"Hours\", y=\"Number of Submissions\",\n",
    "             hue=\"Platform\").set(title=\"Hourly Submissions Trend\")\n",
    "fig.savefig('hourly-submissions-trend.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055677ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['time_stamp'] = pd.to_datetime(\n",
    "    twitter_data['time_stamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "mask = (twitter_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (\n",
    "    twitter_data['time_stamp'] < datetime(2022, 11, 21, 0, 0, 0).isoformat())\n",
    "t_data = twitter_data.loc[mask]\n",
    "t_data = t_data['time_stamp'].value_counts().rename_axis(\n",
    "    'Date').reset_index(name='Number of Submissions')\n",
    "t_data = t_data.sort_values(by='Date')\n",
    "t_data.reset_index(drop=True)\n",
    "t_data = t_data.groupby([t_data['Date'].dt.floor('d')])\n",
    "t_data = t_data[\"Number of Submissions\"].sum()\n",
    "t_data = DataFrame(t_data).reset_index()\n",
    "platform = ['twitter' for _ in range(len(t_data))]\n",
    "t_data['Platform'] = platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['time_stamp'] = pd.to_datetime(\n",
    "    reddit_data['time_stamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "mask = (reddit_data['time_stamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (\n",
    "    reddit_data['time_stamp'] < datetime(2022, 11, 21, 0, 0, 0).isoformat())\n",
    "r_data = reddit_data.loc[mask]\n",
    "r_data = r_data['time_stamp'].value_counts().rename_axis(\n",
    "    'Date').reset_index(name='Number of Submissions')\n",
    "r_data = r_data.sort_values(by='Date')\n",
    "r_data.reset_index(drop=True)\n",
    "r_data = r_data.groupby([r_data['Date'].dt.floor('d')])\n",
    "r_data = r_data[\"Number of Submissions\"].sum()\n",
    "r_data = DataFrame(r_data).reset_index()\n",
    "platform = ['reddit' for _ in range(len(r_data))]\n",
    "r_data['Platform'] = platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_data['comment_timestamp'] = pd.to_datetime(\n",
    "    youtube_data['comment_timestamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "mask = (youtube_data['comment_timestamp'] >= datetime(2022, 11, 1, 0, 0, 0).isoformat()) & (\n",
    "    youtube_data['comment_timestamp'] < datetime(2022, 11, 21, 0, 0, 0).isoformat())\n",
    "y_data = youtube_data.loc[mask]\n",
    "y_data = y_data['comment_timestamp'].value_counts().rename_axis(\n",
    "    'Date').reset_index(name='Number of Submissions')\n",
    "y_data = y_data.sort_values(by='Date')\n",
    "y_data.reset_index(drop=True)\n",
    "y_data = y_data.groupby([y_data['Date'].dt.floor('d')])\n",
    "y_data = y_data[\"Number of Submissions\"].sum()\n",
    "y_data = DataFrame(y_data).reset_index()\n",
    "platform = ['youtube' for _ in range(len(y_data))]\n",
    "y_data['Platform'] = platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([t_data, r_data, y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18122ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10.00, 5.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=final, x=\"Date\", y=\"Number of Submissions\",\n",
    "             hue=\"Platform\").set(title=\"Daily Submissions Trend\")\n",
    "fig.savefig('daily-submissions-trend.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf5c64",
   "metadata": {},
   "source": [
    "# Required Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets_count = pd.read_csv('tweets_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1264cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10.00, 5.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=total_tweets_count, x=\"Date\", y=\"Number of Tweets\").set(\n",
    "    title=\"Total Number of Tweets\")\n",
    "fig.savefig('total-tweets.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['time_stamp'] = pd.to_datetime(\n",
    "    reddit_data['time_stamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "mask = (reddit_data['time_stamp'] >= datetime(2022, 11, 4, 0, 0, 0).isoformat()) & (\n",
    "    reddit_data['time_stamp'] < datetime(2022, 11, 15, 0, 0, 0).isoformat())\n",
    "r_data = reddit_data.loc[mask]\n",
    "r_data = r_data['time_stamp'].value_counts().rename_axis(\n",
    "    'Date').reset_index(name='Number of Submissions')\n",
    "r_data = r_data.sort_values(by='Date')\n",
    "r_data.reset_index(drop=True)\n",
    "r_data = r_data.groupby([r_data['Date'].dt.floor('H')])\n",
    "r_data = r_data[\"Number of Submissions\"].sum()\n",
    "r_data = DataFrame(r_data).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10.00, 5.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=r_data, x=\"Date\", y=\"Number of Submissions\").set(\n",
    "    title=\"Total Number of Submissions\")\n",
    "fig.savefig('total-subreddit-submissions.pdf', dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
